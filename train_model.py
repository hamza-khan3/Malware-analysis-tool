import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
import joblib
import matplotlib.pyplot as plt

def load_and_preprocess_data(file_path):
    df = pd.read_csv(file_path)
    X = df.drop('Label', axis=1)
    y = df['Label'].apply(lambda x: 1 if x == 'Malicious' else 0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    return X_train, X_test, y_train, y_test, scaler

def feature_selection(X_train, y_train):
    rf = RandomForestClassifier(n_estimators=50, random_state=1)
    rfe = RFE(estimator=rf, n_features_to_select=10, step=1)
    rfe.fit(X_train, y_train)
    return rfe

def build_keras_model(input_dim):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_dim,)),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def experiment_with_ensemble(X_train, y_train, X_test, y_test):
    rf = RandomForestClassifier(n_estimators=50, random_state=1)
    lr = LogisticRegression()
    svm = SVC(probability=True)
    nn = KerasClassifier(build_fn=build_keras_model, input_dim=X_train.shape[1], epochs=20, batch_size=32, verbose=0)

    ensemble = VotingClassifier(estimators=[
        ('rf', rf), ('lr', lr), ('svm', svm), ('nn', nn)
    ], voting='soft')

    ensemble.fit(X_train, y_train)
    predictions = ensemble.predict(X_test)
    print("Ensemble Classification Report:")
    print(classification_report(y_test, predictions))
    print("ROC AUC Score:", roc_auc_score(y_test, predictions))

def hyperparameter_tuning(X_train, y_train):
    param_grid = {
        'rf__n_estimators': [50, 100, 200],
        'lr__C': [0.1, 1.0, 10.0],
        'svm__C': [0.1, 1.0, 10.0]
    }

    rf = RandomForestClassifier(random_state=1)
    lr = LogisticRegression()
    svm = SVC(probability=True)
    nn = KerasClassifier(build_fn=build_keras_model, input_dim=X_train.shape[1], epochs=20, batch_size=32, verbose=0)

    ensemble = VotingClassifier(estimators=[
        ('rf', rf), ('lr', lr), ('svm', svm), ('nn', nn)
    ], voting='soft')

    grid = GridSearchCV(estimator=ensemble, param_grid=param_grid, cv=5, scoring='accuracy')
    grid.fit(X_train, y_train)
    print("Best parameters found: ", grid.best_params_)
    return grid.best_estimator_

def main():
    file_path = 'malware_features.csv'
    X_train, X_test, y_train, y_test, scaler = load_and_preprocess_data(file_path)
    
    # Feature selection
    rfe = feature_selection(X_train, y_train)
    X_train = rfe.transform(X_train)
    X_test = rfe.transform(X_test)

    # Hyperparameter tuning
    best_model = hyperparameter_tuning(X_train, y_train)

    # Experiment with ensemble models
    experiment_with_ensemble(X_train, y_train, X_test, y_test)

    # Save the best model
    joblib.dump(scaler, 'scaler.pkl')
    joblib.dump(rfe, 'rfe.pkl')
    joblib.dump(best_model, 'best_ensemble_model.pkl')
    print("Model training completed and saved as 'best_ensemble_model.pkl'.")

if __name__ == "__main__":
    main()
